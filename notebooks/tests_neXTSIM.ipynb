{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbdb46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.functional as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "##Adding src path to get our code\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils.graph_utils import interpolate_element_into_nodes, bin_to_torchGraph\n",
    "from models.GCNN_node import GCNN_node\n",
    "from datasets.Ice_graph_dataset import Ice_graph_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test notebook to try ideas and explain them \n",
    "\n",
    "Some of the implemented functions are loaded and demostrated in the following lines. \n",
    "Later The code is presented for testing. \n",
    "\n",
    "Note the main code is in /src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db903b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_graphs = [dict(np.load(f'../data/{file}')) for file in sorted(os.listdir('../data')) if file[-3:]=='npz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating features from element to nodes...: 100%|██████████| 25/25 [00:27<00:00,  1.11s/it]\n",
      "Converting bins to torch graphs...: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0514, 0.2110], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Interpolate element information to nodes. #To be changed!\n",
    "features_interp = list(zip(\n",
    "    ['Ci_node','Thickness_node','Damage_node','S0_node','S1_node'],\n",
    "    ['Concentration','Thickness','Damage','Sigma_0','Sigma_1']\n",
    ")) \n",
    "file_graphs = interpolate_element_into_nodes(file_graphs,features_interp)\n",
    "\n",
    "#Create torch graphs based on desired features and a given target index \n",
    "feature_list = ['M_VT_x', 'M_VT_y', 'M_wind_x', 'M_wind_y', 'M_ocean_x', 'M_ocean_y', 'x', 'y', 'Ci_node', 'Thickness_node', 'Damage_node' ]\n",
    "target_index=3030\n",
    "graph_list = bin_to_torchGraph(file_graphs,feature_list,target_index)\n",
    "\n",
    "#Create a torch dataset to store/retrieve/transform the data\n",
    "ice_dataset = Ice_graph_dataset(graph_list)\n",
    "#get an instance to try our toy model\n",
    "example_graph = next(iter(ice_dataset))\n",
    "\n",
    "#Load a GNN and test the graphs on it\n",
    "num_features = example_graph.x.shape[-1]  # Node feature dimension\n",
    "hidden_channels = 6\n",
    "num_classes = 2  #x and y\n",
    "\n",
    "model = GCNN_node(num_features, hidden_channels, num_classes)\n",
    "\n",
    "# test forward pass\n",
    "output = model(example_graph.x, example_graph.edge_index,example_graph.edge_attr)\n",
    "\n",
    "# Print the output (lat, lon tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damage (137736,)\n",
      "Concentration (137736,)\n",
      "Thickness (137736,)\n",
      "Sigma_0 (137736,)\n",
      "Sigma_1 (137736,)\n",
      "M_VT_x (70886,)\n",
      "M_VT_y (70886,)\n",
      "M_wind_x (70886,)\n",
      "M_wind_y (70886,)\n",
      "M_ocean_x (70886,)\n",
      "M_ocean_y (70886,)\n",
      "x (70886,)\n",
      "y (70886,)\n",
      "t (137736, 3)\n",
      "i (70886,)\n",
      "Ci_node (70886,)\n",
      "Thickness_node (70886,)\n",
      "Damage_node (70886,)\n",
      "S0_node (70886,)\n",
      "S1_node (70886,)\n",
      "sum_elements (70886,)\n"
     ]
    }
   ],
   "source": [
    "for i,item in file_graphs[12].items():\n",
    "    print(i,item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating Ci from element to nodes...: 100%|██████████| 25/25 [00:31<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(file_graphs,\"Interpolating Ci from element to nodes...\"):\n",
    "\n",
    "    file['Ci_node'] = np.zeros(file['x'].shape)\n",
    "    file['Thickness_node'] = np.zeros(file['x'].shape)\n",
    "    file['Damage_node'] = np.zeros(file['x'].shape)\n",
    "    file['S0_node'] = np.zeros(file['x'].shape)\n",
    "    file['S1_node'] = np.zeros(file['x'].shape)\n",
    "\n",
    "    file['sum_elements'] = np.zeros(file['x'].shape)\n",
    "\n",
    "\n",
    "    for i,element in enumerate(file['t']):\n",
    "        file['Ci_node'][element] += file['Concentration'][i]\n",
    "        file['Thickness_node'][element] += file['Thickness'][i]\n",
    "        file['Damage_node'][element] += file['Damage'][i]\n",
    "        file['S0_node'][element] += file['Sigma_0'][i]\n",
    "        file['S1_node'][element] += file['Sigma_1'][i]\n",
    "        \n",
    "        file['sum_elements'][element] += 1\n",
    "\n",
    "\n",
    "    file['Ci_node'] = file['Ci_node']/file['sum_elements']\n",
    "    file['Thickness_node'] = file['Thickness_node']/file['sum_elements']\n",
    "    file['Damage_node'] = file['Damage_node']/file['sum_elements']\n",
    "    file['S0_node'] = file['S0_node']/file['sum_elements']\n",
    "    file['S1_node'] = file['S1_node']/file['sum_elements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node vs Element distributions\n",
      "\n",
      "Concentration distributions\n",
      "\tStd:  0.2774466430714578 - 0.2770468\n",
      "\tMean:  0.8938239535221482 - 0.8954742\n",
      "Thickness distributions\n",
      "\tStd:  0.6762332596798135 - 0.67523193\n",
      "\tMean:  1.2166780835472275 - 1.2214028\n",
      "Damage distributions\n",
      "\tStd:  0.10938263753400297 - 0.13377665\n",
      "\tMean:  0.03525301232316462 - 0.03522985\n",
      "Sigma_0 distributions\n",
      "\tStd:  2678.028312898291 - 3091.5708\n",
      "\tMean:  -1527.0704480992035 - -1541.5564\n",
      "Sigma_1 distributions\n",
      "\tStd:  2334.660421478878 - 2658.9407\n",
      "\tMean:  1743.758830423752 - 1768.2036\n"
     ]
    }
   ],
   "source": [
    "# Do we preserve the same distribution?\n",
    "one_file = file_graphs[12]\n",
    "\n",
    "node_vars = ['Ci_node','Thickness_node','Damage_node','S0_node','S1_node']\n",
    "element_vars = ['Concentration','Thickness','Damage','Sigma_0','Sigma_1']\n",
    "\n",
    "print(\"Node vs Element distributions\\n\")\n",
    "for n,e in zip(node_vars,element_vars):\n",
    "    print(f\"{e} distributions\")\n",
    "    print(\"\\tStd: \", one_file[n].std(),\"-\",one_file[e].std())\n",
    "    print(\"\\tMean: \", one_file[n].mean(),\"-\",one_file[e].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8aacc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting bins to graphs...: 24it [00:18,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "#Compute per hour:\n",
    "#node features [num_nodes, num_node_features]\n",
    "#edges [2, num_edges]\n",
    "#edges atributtes [num_edges, num_edge_features]\n",
    "graph_list=[]\n",
    "target_idx = 15000\n",
    "feature_list = ['M_VT_x', 'M_VT_y', 'M_wind_x', 'M_wind_y', 'M_ocean_x', 'M_ocean_y', 'x', 'y', 'Ci_node', 'Thickness_node', 'Damage_node' ]\n",
    "\n",
    "for i,hour_graph in tqdm(enumerate(file_graphs[:-1]),\"Converting bins to graphs...\"):\n",
    "    #get the next pos of target node\n",
    "    target_coords = torch.tensor([file_graphs[i+1]['x'][target_idx],file_graphs[i+1]['y'][target_idx]])\n",
    "\n",
    "    #concat all node features sequentially (following index(t) number) in a tensor\n",
    "    features = []\n",
    "   \n",
    "    idx_counter=0\n",
    "    idx_list = []\n",
    "    for key,item in hour_graph.items():\n",
    "\n",
    "        if key in feature_list:\n",
    "            idx_list.append(key)\n",
    "            features.append(torch.tensor(np.array([item])))\n",
    "        \n",
    "\n",
    "    node_features = torch.cat(features).t().to(torch.float32)\n",
    "\n",
    "    #find all distinct (undirected) edges from every triangle\n",
    "    edges = []\n",
    "    for triangle in hour_graph['t']:\n",
    "        edges += [ tuple(triangle[:2]), tuple(triangle[1:]), tuple(triangle[[0,-1]]) ] #tuples since we set() it later\n",
    "\n",
    "    # Get a unique set and convert it to a torch tensor\n",
    "    edges = torch.tensor(list(set(edges))).t()\n",
    "\n",
    "    #Now we need to consult x,y coordinates of each node of the edges and compute the edge distance\n",
    "    # -3,-4 index correspond to x,y in the feature map, for each each row of edge ends we retrieve this info by index\n",
    "    # and we stack it as a 2xE (2 for each edge end, E as number of edges)\n",
    "  \n",
    "    coord_idx= [i for i,key in enumerate(idx_list) if key in ['x','y']]\n",
    "    if len(coord_idx)==2:\n",
    "        edges_coordinates = [\n",
    "            torch.stack(\n",
    "                [\n",
    "                    node_features[edge_row][:,coord_idx[0]],\n",
    "                    node_features[edge_row][:,coord_idx[1]]\n",
    "                ]\n",
    "            )\n",
    "            for edge_row in edges\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"Unable to find coordinates for nodes in graph mesh. \\nDid you include it in the feature list?\")\n",
    "    #now we can compute the norm of each edge vector using torch api\n",
    "    # unsqueeze to match [num_edges, num_edge_features] shape\n",
    "    edge_attr = torch.norm(edges_coordinates[1] - edges_coordinates[0],dim=0).unsqueeze(dim=-1).to(torch.float32)\n",
    "\n",
    "    #Now we can create our torch-geometric graph using the \"Data\" class\n",
    "    ice_graph = Data(x=node_features, edge_index=edges, edge_attr=edge_attr, y=target_coords)\n",
    "    \n",
    "    graph_list.append(ice_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get std,mean for graph data. z-score  vs implemented minmax? instance vs whole data normalization? WIP\n",
    "\n",
    "x_std = np.array(\n",
    "    [i.x.var(dim=0) for i in graph_list]\n",
    ").mean(axis=0)**2\n",
    "\n",
    "edge_attr_std = np.array(\n",
    "    [i.edge_attr.var(dim=0) for i in graph_list]\n",
    ").mean(axis=0)**2\n",
    "\n",
    "\n",
    "x_mean= np.array(\n",
    "    [i.x.mean(dim=0) for i in graph_list]\n",
    ").mean(axis=0)\n",
    "\n",
    "edge_attr_mean = np.array(\n",
    "    [i.edge_attr.mean(dim=0) for i in graph_list]\n",
    ").mean(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31d8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "class Ice_graph_dataset(Dataset):\n",
    "    def __init__(self,data_list):\n",
    "        super(Ice_graph_dataset, self).__init__()\n",
    "\n",
    "        # Define multiple instances of Data objects\n",
    "        self.data_list = data_list\n",
    "\n",
    "        self.transform = NormalizeFeatures(attrs=['x','edge_attr','y'])\n",
    "\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "\n",
    "        data = self.transform(self.data_list[idx])\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "205eda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Ice_graph_dataset(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f506acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5000, dtype=torch.float64), tensor(770845.0310, dtype=torch.float64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset)).y.mean(),graph_list[5].y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aa4e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 141197.1875, -459843.0938], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, output_size):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fc = nn.Linear(hidden_channels, output_size)  # Output layer with 2 neurons\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index,edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index,edge_attr)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Global pooling to aggregate node features (... not sure how elegant)\n",
    "        x = torch.mean(x, dim=0)\n",
    "\n",
    "        # Fully connected layer for the final output\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "num_features = ice_graph.x.shape[-1]  # Node feature dimension\n",
    "hidden_channels = 6\n",
    "num_classes = 2  #latitude and longitude\n",
    "\n",
    "model = GCN(num_features, hidden_channels, num_classes)\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "output = model(ice_graph.x, ice_graph.edge_index,ice_graph.edge_attr)\n",
    "\n",
    "# Print the output (lat, lon tensor)\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextsim_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
